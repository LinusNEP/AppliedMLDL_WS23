{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/feature_engineer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers some libraries for feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data in a Stratified Fashion in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, after using scikit-learn's `train_test_split`, the proportion of values in the sample will be different from the proportion of values in the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 50, 50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37, 34, 41])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get count of each class in the train set\n",
    "\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 16,  9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get count of each class in the test set\n",
    "\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to keep the proportion of classes in the sample the same as the proportion of classes in the entire dataset, add `stratify=y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37, 37, 38])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 13, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy to Prevent Data Leakage in Time-correlated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you randomly split time-correlated datasets for machine learning models, your training set may contain future transactions, leading to biased predictions.\n",
    "\n",
    "To avoid data leakage in time-correlated datasets, split the data by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "\n",
    "# Create the example dataset\n",
    "data = {'customer_id': [1, 2, 3, 4, 5],\n",
    "        'amount': [10.00, 20.00, 15.00, 25.00, 30.00],\n",
    "        'date': ['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-b01de5e433b7>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-b01de5e433b7>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print(f'Train data:\\n{train_data}')\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data randomly into training and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f'Train data:\\n{train_data}')\n",
    "print(f'Test data:\\n{test_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cutoff date\n",
    "cutoff_date = datetime(2021, 1, 4)\n",
    "\n",
    "# Split the data into training and test sets by time\n",
    "train_data = df[df['date'] < cutoff_date]\n",
    "test_data = df[df['date'] >= cutoff_date]\n",
    "\n",
    "print(f'Train data:\\n{train_data}')\n",
    "print(f'Test data:\\n{test_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:01:38.560511Z",
     "start_time": "2021-11-26T14:01:33.638327Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install feature_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove the correlated variables from a dataframe, use `feature_engine.DropCorrelatedFeatures`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:39:16.566622Z",
     "start_time": "2021-11-26T14:39:16.525385Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "\n",
    "# make dataframe with some correlated variables\n",
    "X, y = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=6,\n",
    "        n_redundant=3,\n",
    "        n_clusters_per_class=1,\n",
    "        class_sep=2,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "# trabsform arrays into pandas df and series\n",
    "colnames = [\"var_\" + str(i) for i in range(6)]\n",
    "X = pd.DataFrame(X, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:39:18.295218Z",
     "start_time": "2021-11-26T14:39:18.279783Z"
    }
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:46:16.976588Z",
     "start_time": "2021-11-26T14:46:16.944568Z"
    }
   },
   "outputs": [],
   "source": [
    "X[[\"var_0\", \"var_1\", \"var_2\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the variables with a correlation above 0.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:39:20.375369Z",
     "start_time": "2021-11-26T14:39:20.350873Z"
    }
   },
   "outputs": [],
   "source": [
    "tr = DropCorrelatedFeatures(variables=None, method=\"pearson\", threshold=0.8)\n",
    "\n",
    "Xt = tr.fit_transform(X)\n",
    "\n",
    "tr.correlated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:39:21.274540Z",
     "start_time": "2021-11-26T14:39:21.259715Z"
    }
   },
   "outputs": [],
   "source": [
    "Xt.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.readthedocs.io/en/1.1.x/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Rare Labels with Feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with features with high cardinality, you might want to mark the rare categories as \"Other\". Feature-engine's `RareLabelEncoder` makes it easy for you to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "data = fetch_openml('dating_profile')['data']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = data.dropna(subset=['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, \n",
    "- `tol` species the minimum frequency below which a category is considered rare. \n",
    "- `replace_with` species the value to be used to replace rare categories.\n",
    "- `variables` specify the list of categorical variables that will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RareLabelEncoder(tol=0.05, variables=[\"education\"], replace_with=\"Other\")\n",
    "encoded = encoder.fit_transform(processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the rare categories in the column `education` are replaced with \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded['education'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.readthedocs.io/en/1.1.x/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Data Using Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T13:49:31.177228Z",
     "start_time": "2022-06-03T13:49:28.342740Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, count or frequency can be useful features for your model. If you want to replace categories by either the count or the percentage of observations per category, use feature_engine's `CountFrequencyEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T14:06:26.301980Z",
     "start_time": "2022-06-03T14:06:26.174654Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data[\"price\"], random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, I encode `color` and `clarity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T14:06:28.940601Z",
     "start_time": "2022-06-03T14:06:28.902323Z"
    }
   },
   "outputs": [],
   "source": [
    "# initiate an encoder\n",
    "encoder = CountFrequencyEncoder(\n",
    "    encoding_method=\"frequency\", variables=[\"color\", \"clarity\"]\n",
    ")\n",
    "\n",
    "# fit the encoder\n",
    "encoder.fit(X_train)\n",
    "\n",
    "# process the data\n",
    "p_train = encoder.transform(X_train)\n",
    "p_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T14:06:33.457049Z",
     "start_time": "2022-06-03T14:06:33.416923Z"
    }
   },
   "outputs": [],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.readthedocs.io/en/1.1.x/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return a DataFrame When Using a scikit-learn's Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T14:01:38.560511Z",
     "start_time": "2021-11-26T14:01:33.638327Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install feature_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a scikit-learn's transformer on your DataFrame will return a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T15:07:54.269148Z",
     "start_time": "2022-02-14T15:07:54.261783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T15:08:37.558864Z",
     "start_time": "2022-02-14T15:08:37.512305Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to return a pandas DataFrame instead, use feature-engine's `SklearnTransformerWrapper` along with your scikit-learn's tranformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T15:16:55.548188Z",
     "start_time": "2022-02-14T15:16:55.494583Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = SklearnTransformerWrapper(transformer=StandardScaler())\n",
    "scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.readthedocs.io/en/1.1.x/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Encoding for Dirty Categories Using dirty_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T14:25:00.174790Z",
     "start_time": "2021-12-01T14:24:54.478018Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install dirty-cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the similarities among dirty categories when encoding categorical variables, use dirty_cat’s `SimilarityEncoder` . \n",
    "\n",
    "To understand how `SimilarityEncoder` works, let's start with the employee_salaries dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T14:28:52.846093Z",
     "start_time": "2021-12-01T14:28:52.753694Z"
    }
   },
   "outputs": [],
   "source": [
    "from dirty_cat.datasets import fetch_employee_salaries\n",
    "from dirty_cat import SimilarityEncoder\n",
    "\n",
    "X = fetch_employee_salaries().X\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T14:44:56.498071Z",
     "start_time": "2021-12-01T14:44:56.489775Z"
    }
   },
   "outputs": [],
   "source": [
    "dirty_column = \"employee_position_title\"\n",
    "X_dirty = df[dirty_column].values\n",
    "X_dirty[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that titles such as 'Master Police Officer' and 'Police Officer III' are similar. We can use `SimilaryEncoder` to encode these categories while capturing their similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T14:38:40.668937Z",
     "start_time": "2021-12-01T14:38:40.635484Z"
    }
   },
   "outputs": [],
   "source": [
    "enc = SimilarityEncoder(similarity=\"ngram\")\n",
    "X_enc = enc.fit_transform(X_dirty[:10].reshape(-1, 1))\n",
    "X_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Let's create a heatmap to understand the correlation between the encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T14:46:56.997543Z",
     "start_time": "2021-12-01T14:46:56.985760Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "def plot_similarity(labels, features):\n",
    "  \n",
    "    normalized_features = normalize(features)\n",
    "    \n",
    "    # Create correction matrix\n",
    "    corr = np.inner(normalized_features, normalized_features)\n",
    "    \n",
    "    # Plot\n",
    "    figsize(10, 10)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(corr, xticklabels=labels, yticklabels=labels, vmin=0,\n",
    "        vmax=1, cmap=\"YlOrRd\", annot=True, annot_kws={\"size\": 10})\n",
    "        \n",
    "    g.set_xticklabels(labels, rotation=90)\n",
    "    g.set_title(\"Similarity\")\n",
    "\n",
    "\n",
    "def encode_and_plot(labels):\n",
    "  \n",
    "    enc = SimilarityEncoder(similarity=\"ngram\") # Encode\n",
    "    X_enc = enc.fit_transform(labels.reshape(-1, 1))\n",
    "    \n",
    "    plot_similarity(labels, X_enc) # Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T14:46:58.531298Z",
     "start_time": "2021-12-01T14:46:57.838439Z"
    }
   },
   "outputs": [],
   "source": [
    "encode_and_plot(X_dirty[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the matrix above,\n",
    "- The similarity between the same strings such as 'Office Services Coordinator' and 'Office Services Coordinator' is 1\n",
    "- The similarity between somewhat similar strings such as 'Office Services Coordinator' and 'Master Police Officer' is 0.41\n",
    "- The similarity between two very different strings such as 'Social Worker IV' and 'Polic Aide' is 0.028\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to dirty-cat](https://dirty-cat.github.io/).\n",
    "\n",
    "[Link to my full article about dirty-cat](https://towardsdatascience.com/similarity-encoding-for-dirty-categories-using-dirty-cat-d9f0b581a552)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snorkel — Programmatically Build Training Data in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T14:39:23.498178Z",
     "start_time": "2022-02-25T14:39:06.427037Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you try to determine whether a job posting is fake or not. You come up with some assumptions about a fake job posting, such as:\n",
    "* If a job posting has few to no descriptions about the requirements, it is likely to be fake.\n",
    "* If a job posting does not include any company profile or logo, it is likely to be fake.\n",
    "* If the job posting requires some sort of education or experience, it is likely to be real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T14:56:21.318677Z",
     "start_time": "2022-02-25T14:56:19.057187Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "train_df = pd.read_pickle(\n",
    "    \"https://github.com/khuyentran1401/Data-science/blob/master/feature_engineering/snorkel_example/train_fake_jobs.pkl?raw=true\"\n",
    ")\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How do you test which of these features are the most accurate in predicting fraud?\n",
    "\n",
    "That is when Snorkel comes in handy. Snorkel is an open-source Python library for programmatically building training datasets without manual labeling. \n",
    "\n",
    "To learn how Snorkel works, start with giving a meaningful name to each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T14:47:33.879404Z",
     "start_time": "2022-02-25T14:47:33.858590Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "\n",
    "FAKE = 1\n",
    "REAL = 0\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that:\n",
    "- Fake companies don’t have company profiles or logos\n",
    "- Fake companies are found in a lot of fake job postings\n",
    "- Real job postings often requires a certain level of experience and education \n",
    "\n",
    "Let’s test those assumptions using Snorkel’s `labeling_function` decorator. The `labeling_function` decorator allows us to quickly label instances in a dataset using functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T15:14:38.573702Z",
     "start_time": "2022-02-25T15:14:38.530583Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def no_company_profile(x: pd.Series):\n",
    "    return FAKE if x.company_profile == \"\" else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def no_company_logo(x: pd.Series):\n",
    "    return FAKE if x.has_company_logo == 0 else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def required_experience(x: pd.Series):\n",
    "    return REAL if x.required_experience else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def required_education(x: pd.Series):\n",
    "    return REAL if x.required_education else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ABSTAIN` or `-1` tells Snorkel not to make any conclusion about the instance that doesn’t satisfy the condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use each of these labeling functions to label our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T15:14:52.178397Z",
     "start_time": "2022-02-25T15:14:49.753200Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    no_company_profile,\n",
    "    no_company_logo,\n",
    "    required_experience,\n",
    "    required_education,\n",
    "]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the labels using each labeling function, we can use `LFAnalysis` to determine the accuracy of these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T15:14:52.888783Z",
     "start_time": "2022-02-25T15:14:52.655406Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary(Y=train_df.fraudulent.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the statistics in the table above:\n",
    "* **Polarity**: The set of unique labels this LF outputs (excluding abstains)\n",
    "* **Coverage**: The fraction of the dataset that is labeled\n",
    "* **Overlaps**: The fraction of the dataset where this LF and at least one other LF agree\n",
    "* **Conflicts**: The fraction of the dataset where this LF and at least one other LF disagree\n",
    "* **Correct**: The number of data points this LF labels correctly\n",
    "* **Incorrect**: The number of data points this LF labels incorrectly\n",
    "* **Empirical** Accuracy: The empirical accuracy of this LF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to Snorkel](https://www.snorkel.org/).\n",
    "\n",
    "[My full article about Snorkel](https://towardsdatascience.com/snorkel-programmatically-build-training-data-in-python-712fc39649fe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sketch: AI Code-Writing Assistant That Understands Data Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wouldn't it be nice if you could get insights into your data by simply asking a question? Sketch allows you to do exactly that.\n",
    "\n",
    "Sketch is an AI code-writing assistant for pandas users that understands the context of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "import sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('taxis')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.ask(\n",
    "    \"Can you give me friendly names for each column?\" \n",
    "    \"(Output as an HTML list)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.ask(\n",
    "    \"Which payment is the most popular payment?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.howto(\"Create some features from the pickup column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new column for the hour of the pickup\n",
    "data['pickup_hour'] = data['pickup'].dt.hour\n",
    "\n",
    "# Create a new column for the day of the week of the pickup\n",
    "data['pickup_day'] = data['pickup'].dt.weekday\n",
    "\n",
    "# Create a new column for the month of the pickup\n",
    "data['pickup_month'] = data['pickup'].dt.month_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.howto(\n",
    "    \"Create some features from the pickup_zone column\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new column called 'pickup_zone_count'\n",
    "data['pickup_zone_count'] = data.groupby('pickup_zone')['pickup_zone'].transform('count')\n",
    "\n",
    "# Create a new column called 'pickup_zone_fare'\n",
    "data['pickup_zone_fare'] = data.groupby('pickup_zone')['fare'].transform('mean')\n",
    "\n",
    "# Create a new column called 'pickup_zone_distance'\n",
    "data['pickup_zone_distance'] = data.groupby('pickup_zone')['distance'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to sketch](https://github.com/approximatelabs/sketch)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c3bc044b9863ed6dec4c55e7ad5af27f030f7d27aed3f39d7a4886a926c4e2c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
